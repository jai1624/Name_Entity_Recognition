{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","mount_file_id":"1_8r6Vql7f1ebMMDL-h6t9aUWdx0p3lHq","authorship_tag":"ABX9TyMtpHTgv1JqBhGzV+gOGAPa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"8206dc2c4c0f4d828b5987adfaa93741":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8df75ca12a314ca58a5c32baa9eba144","IPY_MODEL_a0b042d7848a472b858ceeeeb4186138","IPY_MODEL_b65e1a92b8554c718c4c457ef7f42efe"],"layout":"IPY_MODEL_b44e2cb0c4ca4472a786b7e14310ed95"}},"8df75ca12a314ca58a5c32baa9eba144":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e1636f1bce6484eba7504dbbc781eaf","placeholder":"​","style":"IPY_MODEL_5dc72c90f8644e5e8e940140fde99a3a","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"a0b042d7848a472b858ceeeeb4186138":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2abaac49ee614d93afccc30d75444be3","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f90147bc72074c19b5bb9cde1f8e7e45","value":231508}},"b65e1a92b8554c718c4c457ef7f42efe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7978e38bfba43d39d1aa185dc8e3f74","placeholder":"​","style":"IPY_MODEL_70f3e5d37e504f0aaa662d572c3e0987","value":" 232k/232k [00:00&lt;00:00, 4.91MB/s]"}},"b44e2cb0c4ca4472a786b7e14310ed95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e1636f1bce6484eba7504dbbc781eaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5dc72c90f8644e5e8e940140fde99a3a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2abaac49ee614d93afccc30d75444be3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f90147bc72074c19b5bb9cde1f8e7e45":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b7978e38bfba43d39d1aa185dc8e3f74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70f3e5d37e504f0aaa662d572c3e0987":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72a6a2d12fc049e9bd59d1bff51b31bf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0de816652887491f8ecbb0fdb5b5a410","IPY_MODEL_21330b93301442c3b41ae0fab63e160b","IPY_MODEL_9930ff994bd94ffca89162ffe8287163"],"layout":"IPY_MODEL_9e67a03284e245418ca94bbeb1774d07"}},"0de816652887491f8ecbb0fdb5b5a410":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2b97c246c9942b9a95bf19fe019a0b1","placeholder":"​","style":"IPY_MODEL_286ed3605050406f8df2cb69a947bf75","value":"Downloading (…)okenizer_config.json: 100%"}},"21330b93301442c3b41ae0fab63e160b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_327782026b294d8c877791ee23addf3e","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_39f1fe4045944df5987823b7912b5488","value":28}},"9930ff994bd94ffca89162ffe8287163":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_20aacd6c1b6d40b081ffea324d06b3b5","placeholder":"​","style":"IPY_MODEL_2f0cddb847f34269af2d9741cd2cd8ba","value":" 28.0/28.0 [00:00&lt;00:00, 1.31kB/s]"}},"9e67a03284e245418ca94bbeb1774d07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2b97c246c9942b9a95bf19fe019a0b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"286ed3605050406f8df2cb69a947bf75":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"327782026b294d8c877791ee23addf3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39f1fe4045944df5987823b7912b5488":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"20aacd6c1b6d40b081ffea324d06b3b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f0cddb847f34269af2d9741cd2cd8ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"44d5fd5fa8ef45f3936656cca57cd6aa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_42b53b0780434013befa9173f340be17","IPY_MODEL_85a88d6383fc411abdd8810b2fb9c1aa","IPY_MODEL_c8fbd34d8fc54e4b983f7dc42a91fd60"],"layout":"IPY_MODEL_926dfe46e7184dae8b4c69916cf71408"}},"42b53b0780434013befa9173f340be17":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea63070e983b4892ac4eba19d1a0dc75","placeholder":"​","style":"IPY_MODEL_3fa06052c08a4398b1e718cced01e5c6","value":"Downloading (…)lve/main/config.json: 100%"}},"85a88d6383fc411abdd8810b2fb9c1aa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff80d71f3fcf40b7a73c31eb39af6f34","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5f41260ce44149c19dc2de4c5c4f9890","value":570}},"c8fbd34d8fc54e4b983f7dc42a91fd60":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ee4a4ca634c4b308523584c58675fe6","placeholder":"​","style":"IPY_MODEL_05c9ad9ed2ce485d83e75d9b83e488da","value":" 570/570 [00:00&lt;00:00, 23.0kB/s]"}},"926dfe46e7184dae8b4c69916cf71408":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea63070e983b4892ac4eba19d1a0dc75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fa06052c08a4398b1e718cced01e5c6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff80d71f3fcf40b7a73c31eb39af6f34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f41260ce44149c19dc2de4c5c4f9890":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9ee4a4ca634c4b308523584c58675fe6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05c9ad9ed2ce485d83e75d9b83e488da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["!pip install transformers seqeval[gpu]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4AZizk5ODNsj","executionInfo":{"status":"ok","timestamp":1687177748536,"user_tz":-330,"elapsed":16587,"user":{"displayName":"Purchase - Techversant Infotech","userId":"04917963250138759673"}},"outputId":"f3acf160-5c4f-4746-821a-2f7f2df65441"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting seqeval[gpu]\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m120.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval[gpu]) (1.2.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (3.1.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16165 sha256=68b3661084e8427b8d188e10be54924b745368db86d54e35fc99d88ec2df59e6\n","  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n","Successfully built seqeval\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers, seqeval\n","Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 seqeval-1.2.2 tokenizers-0.13.3 transformers-4.30.2\n"]}]},{"cell_type":"markdown","source":["# Import Libraries\n"],"metadata":{"id":"ERznsoIiy8j9"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertConfig, BertForTokenClassification"],"metadata":{"id":"yoTs1wqkDXuS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UAwmFUonDgzT","executionInfo":{"status":"ok","timestamp":1686665677210,"user_tz":-330,"elapsed":788,"user":{"displayName":"Purchase - Techversant Infotech","userId":"04917963250138759673"}},"outputId":"9ebf7a98-5548-46b1-9e62-240bcdd5ffc9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"markdown","source":["# Downloading and preprocessing the data"],"metadata":{"id":"maJ593hxo5DH"}},{"cell_type":"code","source":["data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Restaurant_NER/dataset/restaurant-bert.csv\", sep='*',encoding='unicode_escape').fillna(method=\"ffill\")\n","data.head(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"4l_qbQ00Dmyz","executionInfo":{"status":"ok","timestamp":1687177868391,"user_tz":-330,"elapsed":1665,"user":{"displayName":"Purchase - Techversant Infotech","userId":"04917963250138759673"}},"outputId":"5eef6ed7-ded0-4867-f202-1d6745a9314b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Sentence #        Word  POS     Tag\n","0           1        good   JJ  B-FOOD\n","1           1        food   NN  I-FOOD\n","2           1         and   CC       O\n","3           1    friendly   RB  B-RORG\n","4           1     service   NN  I-RORG\n","5           1         the   DT       O\n","6           1  restaurant   NN       O\n","7           1          is  VBZ       O\n","8           1          on   IN       O\n","9           1       upper   JJ       O"],"text/html":["\n","  <div id=\"df-e2870d59-502c-4f01-8c17-c52164eca4c1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence #</th>\n","      <th>Word</th>\n","      <th>POS</th>\n","      <th>Tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>good</td>\n","      <td>JJ</td>\n","      <td>B-FOOD</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>food</td>\n","      <td>NN</td>\n","      <td>I-FOOD</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>and</td>\n","      <td>CC</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>friendly</td>\n","      <td>RB</td>\n","      <td>B-RORG</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>service</td>\n","      <td>NN</td>\n","      <td>I-RORG</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1</td>\n","      <td>the</td>\n","      <td>DT</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1</td>\n","      <td>restaurant</td>\n","      <td>NN</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1</td>\n","      <td>is</td>\n","      <td>VBZ</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1</td>\n","      <td>on</td>\n","      <td>IN</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1</td>\n","      <td>upper</td>\n","      <td>JJ</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e2870d59-502c-4f01-8c17-c52164eca4c1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e2870d59-502c-4f01-8c17-c52164eca4c1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e2870d59-502c-4f01-8c17-c52164eca4c1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["#Let's check how many sentences and words (and corresponding tags) there are in this dataset:\n","data.count()"],"metadata":{"id":"wjap-eJ9DuBn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Let's have a look at the different NER tags, and their frequency:\n","print(\"Number of tags: {}\".format(len(data.Tag.unique())))\n","frequencies = data.Tag.value_counts()\n","frequencies"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"abro71GJpDkG","executionInfo":{"status":"ok","timestamp":1687178622444,"user_tz":-330,"elapsed":596,"user":{"displayName":"Purchase - Techversant Infotech","userId":"04917963250138759673"}},"outputId":"9c342e3b-354d-46ef-fe9c-035fe0fa87c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of tags: 7\n"]},{"output_type":"execute_result","data":{"text/plain":["O         817864\n","B-FOOD     33130\n","B-RORG     27502\n","B-RLOC     19666\n","I-FOOD      7408\n","I-RLOC      6523\n","I-RORG      4209\n","Name: Tag, dtype: int64"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["#There are 3 category tags, each with a \"beginning\" and \"inside\" variant, and the \"outside\" tag. Let's print them by frequency (highest to lowest):\n","tags = {}\n","for tag, count in zip(frequencies.index, frequencies):\n","    if tag != \"O\":\n","        if tag[2:5] not in tags.keys():\n","            tags[tag[2:5]] = count\n","        else:\n","            tags[tag[2:5]] += count\n","    continue\n","\n","print(sorted(tags.items(), key=lambda x: x[1], reverse=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mg6MInN2qPZF","executionInfo":{"status":"ok","timestamp":1687178627667,"user_tz":-330,"elapsed":351,"user":{"displayName":"Purchase - Techversant Infotech","userId":"04917963250138759673"}},"outputId":"327d1405-53b8-4558-882b-9a55f5b8e657"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('FOO', 40538), ('ROR', 31711), ('RLO', 26189)]\n"]}]},{"cell_type":"code","source":["# let's create a new column called \"sentence\" which groups the words by sentence\n","data['sentence'] = data[['Sentence #','Word','Tag']].groupby(['Sentence #'])['Word'].transform(lambda x: ' '.join(x))\n","# let's also create a new column called \"word_labels\" which groups the tags by sentence\n","data['word_labels'] = data[['Sentence #','Word','Tag']].groupby(['Sentence #'])['Tag'].transform(lambda x: ','.join(x))\n","data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":354},"id":"HTPY2McVDzBU","executionInfo":{"status":"ok","timestamp":1687178640401,"user_tz":-330,"elapsed":7344,"user":{"displayName":"Purchase - Techversant Infotech","userId":"04917963250138759673"}},"outputId":"582f7119-c9df-4caa-9816-fc9a2a0fab8b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Sentence #      Word POS     Tag  \\\n","0           1      good  JJ  B-FOOD   \n","1           1      food  NN  I-FOOD   \n","2           1       and  CC       O   \n","3           1  friendly  RB  B-RORG   \n","4           1   service  NN  I-RORG   \n","\n","                                            sentence  \\\n","0  good food and friendly service the restaurant ...   \n","1  good food and friendly service the restaurant ...   \n","2  good food and friendly service the restaurant ...   \n","3  good food and friendly service the restaurant ...   \n","4  good food and friendly service the restaurant ...   \n","\n","                                         word_labels  \n","0  B-FOOD,I-FOOD,O,B-RORG,I-RORG,O,O,O,O,O,O,B-RL...  \n","1  B-FOOD,I-FOOD,O,B-RORG,I-RORG,O,O,O,O,O,O,B-RL...  \n","2  B-FOOD,I-FOOD,O,B-RORG,I-RORG,O,O,O,O,O,O,B-RL...  \n","3  B-FOOD,I-FOOD,O,B-RORG,I-RORG,O,O,O,O,O,O,B-RL...  \n","4  B-FOOD,I-FOOD,O,B-RORG,I-RORG,O,O,O,O,O,O,B-RL...  "],"text/html":["\n","  <div id=\"df-889c2caf-5231-4423-a204-91dd5b19fb6e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence #</th>\n","      <th>Word</th>\n","      <th>POS</th>\n","      <th>Tag</th>\n","      <th>sentence</th>\n","      <th>word_labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>good</td>\n","      <td>JJ</td>\n","      <td>B-FOOD</td>\n","      <td>good food and friendly service the restaurant ...</td>\n","      <td>B-FOOD,I-FOOD,O,B-RORG,I-RORG,O,O,O,O,O,O,B-RL...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>food</td>\n","      <td>NN</td>\n","      <td>I-FOOD</td>\n","      <td>good food and friendly service the restaurant ...</td>\n","      <td>B-FOOD,I-FOOD,O,B-RORG,I-RORG,O,O,O,O,O,O,B-RL...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>and</td>\n","      <td>CC</td>\n","      <td>O</td>\n","      <td>good food and friendly service the restaurant ...</td>\n","      <td>B-FOOD,I-FOOD,O,B-RORG,I-RORG,O,O,O,O,O,O,B-RL...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>friendly</td>\n","      <td>RB</td>\n","      <td>B-RORG</td>\n","      <td>good food and friendly service the restaurant ...</td>\n","      <td>B-FOOD,I-FOOD,O,B-RORG,I-RORG,O,O,O,O,O,O,B-RL...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>service</td>\n","      <td>NN</td>\n","      <td>I-RORG</td>\n","      <td>good food and friendly service the restaurant ...</td>\n","      <td>B-FOOD,I-FOOD,O,B-RORG,I-RORG,O,O,O,O,O,O,B-RL...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-889c2caf-5231-4423-a204-91dd5b19fb6e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-889c2caf-5231-4423-a204-91dd5b19fb6e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-889c2caf-5231-4423-a204-91dd5b19fb6e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["#create dictionary with numbers, indicies\n","label2id = {k: v for v, k in enumerate(data.Tag.unique())}\n","id2label = {v: k for v, k in enumerate(data.Tag.unique())}\n","label2id"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6bwohkhzr2Jl","executionInfo":{"status":"ok","timestamp":1687178761648,"user_tz":-330,"elapsed":343,"user":{"displayName":"Purchase - Techversant Infotech","userId":"04917963250138759673"}},"outputId":"2097c3ac-0ea6-43ee-f673-6c3d56d4ba98"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'B-FOOD': 0,\n"," 'I-FOOD': 1,\n"," 'O': 2,\n"," 'B-RORG': 3,\n"," 'I-RORG': 4,\n"," 'B-RLOC': 5,\n"," 'I-RLOC': 6}"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["#select sentence,labels column and drop duplicates\n","data = data[[\"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n","data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"fqpvtCRIsRUM","executionInfo":{"status":"ok","timestamp":1687178782171,"user_tz":-330,"elapsed":1575,"user":{"displayName":"Purchase - Techversant Infotech","userId":"04917963250138759673"}},"outputId":"50ca6850-ab38-4428-b5c0-c5b5a27d7115"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                            sentence  \\\n","0  good food and friendly service the restaurant ...   \n","1  so we look on the site and it says closes at 1...   \n","2  letic is a nice restaurant tucked away in the ...   \n","3  we all living in phuket usually have our place...   \n","4  definitely one of the best places in bangkok a...   \n","\n","                                         word_labels  \n","0  B-FOOD,I-FOOD,O,B-RORG,I-RORG,O,O,O,O,O,O,B-RL...  \n","1  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n","2  O,O,O,B-RORG,I-RORG,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n","3  O,O,O,B-RLOC,I-RLOC,O,O,O,O,O,B-FOOD,I-FOOD,O,...  \n","4  O,O,O,O,O,O,B-RLOC,I-RLOC,O,O,O,O,O,O,O,O,O,O,...  "],"text/html":["\n","  <div id=\"df-2b38178d-6902-4d12-aa09-278b0b4cfce9\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>word_labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>good food and friendly service the restaurant ...</td>\n","      <td>B-FOOD,I-FOOD,O,B-RORG,I-RORG,O,O,O,O,O,O,B-RL...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>so we look on the site and it says closes at 1...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>letic is a nice restaurant tucked away in the ...</td>\n","      <td>O,O,O,B-RORG,I-RORG,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>we all living in phuket usually have our place...</td>\n","      <td>O,O,O,B-RLOC,I-RLOC,O,O,O,O,O,B-FOOD,I-FOOD,O,...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>definitely one of the best places in bangkok a...</td>\n","      <td>O,O,O,O,O,O,B-RLOC,I-RLOC,O,O,O,O,O,O,O,O,O,O,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2b38178d-6902-4d12-aa09-278b0b4cfce9')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2b38178d-6902-4d12-aa09-278b0b4cfce9 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2b38178d-6902-4d12-aa09-278b0b4cfce9');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["# Cross verify the sentence & label"],"metadata":{"id":"A9SoSmQssYo-"}},{"cell_type":"code","source":["len(data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZVyeLDRtsVAM","executionInfo":{"status":"ok","timestamp":1687178796269,"user_tz":-330,"elapsed":329,"user":{"displayName":"Purchase - Techversant Infotech","userId":"04917963250138759673"}},"outputId":"77123212-4161-4c04-bc2d-35058ef54905"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["18684"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["data.iloc[41].sentence"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"bwJ_EmCKsXnd","executionInfo":{"status":"ok","timestamp":1687178807013,"user_tz":-330,"elapsed":347,"user":{"displayName":"Purchase - Techversant Infotech","userId":"04917963250138759673"}},"outputId":"1987dbe3-e75a-488a-8b85-123eee11709f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'beautiful spot with spectacular food and lovely staff slightly off the beaten track but well worth the journey highly recommend and summarization on 9 restaurant is relaxed vibe'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["data.iloc[41].word_labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"vEbduSrpshqt","executionInfo":{"status":"ok","timestamp":1687178847188,"user_tz":-330,"elapsed":345,"user":{"displayName":"Purchase - Techversant Infotech","userId":"04917963250138759673"}},"outputId":"87d93988-58e1-42af-8e32-74b0ccbe412b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'B-RORG,O,O,O,O,O,B-RORG,B-RORG,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-RORG,B-RLOC'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["# Preparing the dataset & dataloader"],"metadata":{"id":"i7zrQjIUsqL7"}},{"cell_type":"code","source":["MAX_LEN = 128\n","TRAIN_BATCH_SIZE = 4\n","VALID_BATCH_SIZE = 2\n","EPOCHS = 1\n","LEARNING_RATE = 1e-05\n","MAX_GRAD_NORM = 10\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["8206dc2c4c0f4d828b5987adfaa93741","8df75ca12a314ca58a5c32baa9eba144","a0b042d7848a472b858ceeeeb4186138","b65e1a92b8554c718c4c457ef7f42efe","b44e2cb0c4ca4472a786b7e14310ed95","9e1636f1bce6484eba7504dbbc781eaf","5dc72c90f8644e5e8e940140fde99a3a","2abaac49ee614d93afccc30d75444be3","f90147bc72074c19b5bb9cde1f8e7e45","b7978e38bfba43d39d1aa185dc8e3f74","70f3e5d37e504f0aaa662d572c3e0987","72a6a2d12fc049e9bd59d1bff51b31bf","0de816652887491f8ecbb0fdb5b5a410","21330b93301442c3b41ae0fab63e160b","9930ff994bd94ffca89162ffe8287163","9e67a03284e245418ca94bbeb1774d07","c2b97c246c9942b9a95bf19fe019a0b1","286ed3605050406f8df2cb69a947bf75","327782026b294d8c877791ee23addf3e","39f1fe4045944df5987823b7912b5488","20aacd6c1b6d40b081ffea324d06b3b5","2f0cddb847f34269af2d9741cd2cd8ba","44d5fd5fa8ef45f3936656cca57cd6aa","42b53b0780434013befa9173f340be17","85a88d6383fc411abdd8810b2fb9c1aa","c8fbd34d8fc54e4b983f7dc42a91fd60","926dfe46e7184dae8b4c69916cf71408","ea63070e983b4892ac4eba19d1a0dc75","3fa06052c08a4398b1e718cced01e5c6","ff80d71f3fcf40b7a73c31eb39af6f34","5f41260ce44149c19dc2de4c5c4f9890","9ee4a4ca634c4b308523584c58675fe6","05c9ad9ed2ce485d83e75d9b83e488da"]},"id":"mrk_LeJBFYRC","executionInfo":{"status":"ok","timestamp":1687178969367,"user_tz":-330,"elapsed":1288,"user":{"displayName":"Purchase - Techversant Infotech","userId":"04917963250138759673"}},"outputId":"31545e48-93fc-4607-f2e0-9c79a40f068c"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8206dc2c4c0f4d828b5987adfaa93741"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72a6a2d12fc049e9bd59d1bff51b31bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44d5fd5fa8ef45f3936656cca57cd6aa"}},"metadata":{}}]},{"cell_type":"code","source":["def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n","    \"\"\"\n","    Word piece tokenization makes it difficult to match word labels\n","    back up with individual word pieces. This function tokenizes each\n","    word one at a time so that it is easier to preserve the correct\n","    label for each subword. It is, of course, a bit slower in processing\n","    time, but it will help our model achieve higher accuracy.\n","    \"\"\"\n","\n","    tokenized_sentence = []\n","    labels = []\n","\n","    sentence = sentence.strip()\n","\n","    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n","\n","        # Tokenize the word and count # of subwords the word is broken into\n","        tokenized_word = tokenizer.tokenize(word)\n","        n_subwords = len(tokenized_word)\n","\n","        # Add the tokenized word to the final tokenized word list\n","        tokenized_sentence.extend(tokenized_word)\n","\n","        # Add the same label to the new list of labels `n_subwords` times\n","        labels.extend([label] * n_subwords)\n","\n","    return tokenized_sentence, labels"],"metadata":{"id":"FfMdW-QLFfxT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#dataset class (which transforms examples of a dataframe to PyTorch tensors\n","class dataset(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_len):\n","        self.len = len(dataframe)\n","        self.data = dataframe\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __getitem__(self, index):\n","        # step 1: tokenize (and adapt corresponding labels)\n","        sentence = self.data.sentence[index]\n","        word_labels = self.data.word_labels[index]\n","        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n","\n","        # step 2: add special tokens (and corresponding labels)\n","        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n","        labels.insert(0, \"O\") # add outside label for [CLS] token\n","        labels.insert(-1, \"O\") # add outside label for [SEP] token\n","\n","        # step 3: truncating/padding\n","        maxlen = self.max_len\n","\n","        if (len(tokenized_sentence) > maxlen):\n","          # truncate\n","          tokenized_sentence = tokenized_sentence[:maxlen]\n","          labels = labels[:maxlen]\n","        else:\n","          # pad\n","          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n","          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n","\n","        # step 4: obtain the attention mask\n","        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n","\n","        # step 5: convert tokens to input ids\n","        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n","\n","        label_ids = [label2id[label] for label in labels]\n","        # the following line is deprecated\n","        #label_ids = [label if label != 0 else -100 for label in label_ids]\n","\n","        return {\n","              'ids': torch.tensor(ids, dtype=torch.long),\n","              'mask': torch.tensor(attn_mask, dtype=torch.long),\n","              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n","              'targets': torch.tensor(label_ids, dtype=torch.long)\n","        }\n","\n","    def __len__(self):\n","        return self.len"],"metadata":{"id":"wwdcWJQbFxay"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#create 2 datasets, one for training and one for testing. Let's use a 80/20 split\n","train_size = 0.8\n","train_dataset = data.sample(frac=train_size,random_state=200)\n","test_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n","train_dataset = train_dataset.reset_index(drop=True)\n","\n","print(\"FULL Dataset: {}\".format(data.shape))\n","print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n","print(\"TEST Dataset: {}\".format(test_dataset.shape))\n","\n","training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n","testing_set = dataset(test_dataset, tokenizer, MAX_LEN)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hi9usAjfFy0z","executionInfo":{"status":"ok","timestamp":1687179096003,"user_tz":-330,"elapsed":454,"user":{"displayName":"Purchase - Techversant Infotech","userId":"04917963250138759673"}},"outputId":"0010649d-198a-40dc-be79-198b21598892"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["FULL Dataset: (18684, 2)\n","TRAIN Dataset: (14947, 2)\n","TEST Dataset: (3737, 2)\n"]}]},{"cell_type":"code","source":["training_set[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vlQIP3a2F0Oh","executionInfo":{"status":"ok","timestamp":1687179102081,"user_tz":-330,"elapsed":560,"user":{"displayName":"Purchase - Techversant Infotech","userId":"04917963250138759673"}},"outputId":"d1d4d830-7859-430e-becc-919a479656b1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'ids': tensor([  101,  2054,  1037,  6919,  4292,  1037,  7273,  2806, 27387, 22704,\n","          2369,  1037,  2235,  2697,  2007,  7564,  1997,  2300,  3669, 23697,\n","          2015,  1998,  2317, 27083,  3909,  2058,  2009,  1998,  1996,  2833,\n","          2003, 12090,  1045, 16755,  1996, 25482, 20130, 14380, 19673, 16521,\n","          1998,  7680,  7849,  3989,  2006, 14916, 24601, 24728,  2063, 23308,\n","          3347,  2012, 20704, 11921,  5342,  9497, 11460, 24164,  2854,  2011,\n","          2061,  8873,  9834,  2003, 12090,   102,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0]),\n"," 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0]),\n"," 'targets': tensor([2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","         2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2,\n","         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2,\n","         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","         2, 2, 2, 2, 2, 2, 2, 2])}"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["training_set[0][\"ids\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ol0XewWyF32S","executionInfo":{"status":"ok","timestamp":1687179116236,"user_tz":-330,"elapsed":368,"user":{"displayName":"Purchase - Techversant Infotech","userId":"04917963250138759673"}},"outputId":"d921db38-c59e-40f5-a7ec-4368beb5aa48"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([  101,  2054,  1037,  6919,  4292,  1037,  7273,  2806, 27387, 22704,\n","         2369,  1037,  2235,  2697,  2007,  7564,  1997,  2300,  3669, 23697,\n","         2015,  1998,  2317, 27083,  3909,  2058,  2009,  1998,  1996,  2833,\n","         2003, 12090,  1045, 16755,  1996, 25482, 20130, 14380, 19673, 16521,\n","         1998,  7680,  7849,  3989,  2006, 14916, 24601, 24728,  2063, 23308,\n","         3347,  2012, 20704, 11921,  5342,  9497, 11460, 24164,  2854,  2011,\n","         2061,  8873,  9834,  2003, 12090,   102,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0])"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["# print the first 30 tokens and corresponding labels\n","for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0][\"ids\"][:30]), training_set[0][\"targets\"][:30]):\n","  print('{0:10}  {1}'.format(token, id2label[label.item()]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G8ZjidLIF6cC","executionInfo":{"status":"ok","timestamp":1687179119447,"user_tz":-330,"elapsed":391,"user":{"displayName":"Purchase - Techversant Infotech","userId":"04917963250138759673"}},"outputId":"d01a7a7f-901b-4c8a-a37f-207e70e0002d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS]       O\n","what        O\n","a           O\n","wonderful   B-RORG\n","setting     O\n","a           O\n","thai        O\n","style       O\n","pagoda      O\n","nestled     O\n","behind      O\n","a           O\n","small       B-RLOC\n","lake        O\n","with        O\n","plenty      O\n","of          O\n","water       O\n","##li        O\n","##llie      O\n","##s         O\n","and         O\n","white       O\n","cranes      O\n","flying      O\n","over        O\n","it          O\n","and         O\n","the         O\n","food        O\n"]}]},{"cell_type":"code","source":["#define pytorch loaders\n","train_params = {'batch_size': TRAIN_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 0\n","                }\n","\n","test_params = {'batch_size': VALID_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 0\n","                }\n","\n","training_loader = DataLoader(training_set, **train_params)\n","testing_loader = DataLoader(testing_set, **test_params)"],"metadata":{"id":"rgq1m66qF71y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Defining the model\n"],"metadata":{"id":"TWtsHOunttWb"}},{"cell_type":"code","source":["from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AMDKu6YGuHRJ","executionInfo":{"status":"ok","timestamp":1687179263773,"user_tz":-330,"elapsed":560,"user":{"displayName":"Purchase - Techversant Infotech","userId":"04917963250138759673"}},"outputId":"87070f9a-b6e5-42ad-847d-ed14365e0309"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","source":["model = BertForTokenClassification.from_pretrained('bert-base-uncased',\n","                                                   num_labels=len(id2label),\n","                                                   id2label=id2label,\n","                                                   label2id=label2id)\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NWYmVpbQF_Di","executionInfo":{"status":"ok","timestamp":1687179273412,"user_tz":-330,"elapsed":6902,"user":{"displayName":"Purchase - Techversant Infotech","userId":"04917963250138759673"}},"outputId":"236a560b-5f2f-408c-a097-0477e725b207"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForTokenClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",")"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["# Training the model"],"metadata":{"id":"fP5I29UsuP1D"}},{"cell_type":"code","source":["ids = training_set[0][\"ids\"].unsqueeze(0)\n","mask = training_set[0][\"mask\"].unsqueeze(0)\n","targets = training_set[0][\"targets\"].unsqueeze(0)\n","ids = ids.to(device)\n","mask = mask.to(device)\n","targets = targets.to(device)\n","outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n","initial_loss = outputs[0]\n","initial_loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gLuhF-ymGBJC","executionInfo":{"status":"ok","timestamp":1687179379930,"user_tz":-330,"elapsed":3068,"user":{"displayName":"Purchase - Techversant Infotech","userId":"04917963250138759673"}},"outputId":"3546d767-448d-4552-e62d-9ff1f46c1576"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.9041, device='cuda:0', grad_fn=<NllLossBackward0>)"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["#check the shape of batch_size, sequence_length, num_lables\n","tr_logits = outputs[1]\n","tr_logits.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Sl8plymGDpz","executionInfo":{"status":"ok","timestamp":1687179417408,"user_tz":-330,"elapsed":344,"user":{"displayName":"Purchase - Techversant Infotech","userId":"04917963250138759673"}},"outputId":"76482494-d0cb-4bfe-f4c3-1123749178a0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 128, 7])"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"],"metadata":{"id":"31u0zZQvGGRD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Defining the training function on the 80% of the dataset for tuning the bert model\n","def train(epoch):\n","    tr_loss, tr_accuracy = 0, 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","    tr_preds, tr_labels = [], []\n","    # put model in training mode\n","    model.train()\n","\n","    for idx, batch in enumerate(training_loader):\n","\n","        ids = batch['ids'].to(device, dtype = torch.long)\n","        mask = batch['mask'].to(device, dtype = torch.long)\n","        targets = batch['targets'].to(device, dtype = torch.long)\n","\n","        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n","        loss, tr_logits = outputs.loss, outputs.logits\n","        tr_loss += loss.item()\n","\n","        nb_tr_steps += 1\n","        nb_tr_examples += targets.size(0)\n","\n","        if idx % 100==0:\n","            loss_step = tr_loss/nb_tr_steps\n","            print(f\"Training loss per 100 training steps: {loss_step}\")\n","\n","        # compute training accuracy\n","        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n","        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n","        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n","        targets = torch.masked_select(flattened_targets, active_accuracy)\n","        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","\n","        tr_preds.extend(predictions)\n","        tr_labels.extend(targets)\n","\n","        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n","        tr_accuracy += tmp_tr_accuracy\n","\n","        # gradient clipping\n","        torch.nn.utils.clip_grad_norm_(\n","            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n","        )\n","\n","        # backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    epoch_loss = tr_loss / nb_tr_steps\n","    tr_accuracy = tr_accuracy / nb_tr_steps\n","    print(f\"Training loss epoch: {epoch_loss}\")\n","    print(f\"Training accuracy epoch: {tr_accuracy}\")"],"metadata":{"id":"MsNTsiLeGGrC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(EPOCHS):\n","    print(f\"Training epoch: {epoch + 1}\")\n","    train(epoch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UujfqJHlGIqS","executionInfo":{"status":"ok","timestamp":1687179683531,"user_tz":-330,"elapsed":217390,"user":{"displayName":"Purchase - Techversant Infotech","userId":"04917963250138759673"}},"outputId":"c5a99aac-8970-4046-eaa4-147d6ac3b1fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training epoch: 1\n","Training loss per 100 training steps: 1.8862504959106445\n","Training loss per 100 training steps: 0.35634562314146817\n","Training loss per 100 training steps: 0.26139279540201915\n","Training loss per 100 training steps: 0.21450505093779676\n","Training loss per 100 training steps: 0.18435453602166246\n","Training loss per 100 training steps: 0.16299920457744313\n","Training loss per 100 training steps: 0.14638282133331415\n","Training loss per 100 training steps: 0.13329195313835876\n","Training loss per 100 training steps: 0.12371004873261172\n","Training loss per 100 training steps: 0.11523981640874083\n","Training loss per 100 training steps: 0.10786391592646886\n","Training loss per 100 training steps: 0.10156655785662344\n","Training loss per 100 training steps: 0.0963542761989124\n","Training loss per 100 training steps: 0.09158830768353579\n","Training loss per 100 training steps: 0.08743688832505804\n","Training loss per 100 training steps: 0.08379305141458634\n","Training loss per 100 training steps: 0.08033819117238379\n","Training loss per 100 training steps: 0.07724644324694624\n","Training loss per 100 training steps: 0.07442778902467566\n","Training loss per 100 training steps: 0.071827829083029\n","Training loss per 100 training steps: 0.0693826737214645\n","Training loss per 100 training steps: 0.06700312574130299\n","Training loss per 100 training steps: 0.06482974988592874\n","Training loss per 100 training steps: 0.06273461349558625\n","Training loss per 100 training steps: 0.06074252058917704\n","Training loss per 100 training steps: 0.05896167407791297\n","Training loss per 100 training steps: 0.05728878567981011\n","Training loss per 100 training steps: 0.05568903033440241\n","Training loss per 100 training steps: 0.05416104395387402\n","Training loss per 100 training steps: 0.0526155797547238\n","Training loss per 100 training steps: 0.051258861482768266\n","Training loss per 100 training steps: 0.049955281522977193\n","Training loss per 100 training steps: 0.04875661318060706\n","Training loss per 100 training steps: 0.04757064417717254\n","Training loss per 100 training steps: 0.04645033721836099\n","Training loss per 100 training steps: 0.04539733090873151\n","Training loss per 100 training steps: 0.04441446434637977\n","Training loss per 100 training steps: 0.04346085791792824\n","Training loss epoch: 0.04310733894381402\n","Training accuracy epoch: 0.9765490211029481\n"]}]},{"cell_type":"markdown","source":["# Validation Code"],"metadata":{"id":"zX3y60NNxToc"}},{"cell_type":"code","source":["def valid(model, testing_loader):\n","    # put model in evaluation mode\n","    model.eval()\n","\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_examples, nb_eval_steps = 0, 0\n","    eval_preds, eval_labels = [], []\n","\n","    with torch.no_grad():\n","        for idx, batch in enumerate(testing_loader):\n","\n","            ids = batch['ids'].to(device, dtype = torch.long)\n","            mask = batch['mask'].to(device, dtype = torch.long)\n","            targets = batch['targets'].to(device, dtype = torch.long)\n","\n","            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n","            loss, eval_logits = outputs.loss, outputs.logits\n","\n","            eval_loss += loss.item()\n","\n","            nb_eval_steps += 1\n","            nb_eval_examples += targets.size(0)\n","\n","            if idx % 100==0:\n","                loss_step = eval_loss/nb_eval_steps\n","                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n","\n","            # compute evaluation accuracy\n","            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n","            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n","            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n","            targets = torch.masked_select(flattened_targets, active_accuracy)\n","            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","\n","            eval_labels.extend(targets)\n","            eval_preds.extend(predictions)\n","\n","            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n","            eval_accuracy += tmp_eval_accuracy\n","\n","    #print(eval_labels)\n","    #print(eval_preds)\n","\n","    labels = [id2label[id.item()] for id in eval_labels]\n","    predictions = [id2label[id.item()] for id in eval_preds]\n","\n","    #print(labels)\n","    #print(predictions)\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    eval_accuracy = eval_accuracy / nb_eval_steps\n","    print(f\"Validation Loss: {eval_loss}\")\n","    print(f\"Validation Accuracy: {eval_accuracy}\")\n","\n","    return labels, predictions"],"metadata":{"id":"RW8lwSuIGTgD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels, predictions = valid(model, testing_loader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a3EwcbzNI8-E","executionInfo":{"status":"ok","timestamp":1687179747104,"user_tz":-330,"elapsed":43860,"user":{"displayName":"Purchase - Techversant Infotech","userId":"04917963250138759673"}},"outputId":"73375a1c-e754-4648-9320-cfe1342bf36d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation loss per 100 evaluation steps: 0.00021007763280067593\n","Validation loss per 100 evaluation steps: 0.006773041897380752\n","Validation loss per 100 evaluation steps: 0.006799044705225405\n","Validation loss per 100 evaluation steps: 0.007697627075214521\n","Validation loss per 100 evaluation steps: 0.0078703230600357\n","Validation loss per 100 evaluation steps: 0.008080389259042434\n","Validation loss per 100 evaluation steps: 0.007718464253128804\n","Validation loss per 100 evaluation steps: 0.007831641979369908\n","Validation loss per 100 evaluation steps: 0.00770274435854389\n","Validation loss per 100 evaluation steps: 0.007864413472567194\n","Validation loss per 100 evaluation steps: 0.007784455136604631\n","Validation loss per 100 evaluation steps: 0.008020388534175001\n","Validation loss per 100 evaluation steps: 0.00798526837013933\n","Validation loss per 100 evaluation steps: 0.007751428431983508\n","Validation loss per 100 evaluation steps: 0.007791243392285618\n","Validation loss per 100 evaluation steps: 0.007768740527002826\n","Validation loss per 100 evaluation steps: 0.007710226768921746\n","Validation loss per 100 evaluation steps: 0.007844676377042935\n","Validation loss per 100 evaluation steps: 0.007798592990919027\n","Validation Loss: 0.007805155410593958\n","Validation Accuracy: 0.9960791030403915\n"]}]},{"cell_type":"markdown","source":["# Evaluation Metrics"],"metadata":{"id":"cJToo4WDxPjx"}},{"cell_type":"code","source":["from seqeval.metrics import classification_report\n","\n","print(classification_report([labels], [predictions]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7L1uBQfYI-gm","executionInfo":{"status":"ok","timestamp":1687179768515,"user_tz":-330,"elapsed":2885,"user":{"displayName":"Purchase - Techversant Infotech","userId":"04917963250138759673"}},"outputId":"0e945e74-3a16-47bd-afa1-cd3509c94de2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","        FOOD       0.97      0.98      0.98      8369\n","        RLOC       0.98      0.98      0.98      4284\n","        RORG       0.98      0.99      0.99      5759\n","\n","   micro avg       0.98      0.98      0.98     18412\n","   macro avg       0.98      0.98      0.98     18412\n","weighted avg       0.98      0.98      0.98     18412\n","\n"]}]},{"cell_type":"markdown","source":["# Save the model"],"metadata":{"id":"LEb0qHCJxYo6"}},{"cell_type":"code","source":["import os\n","\n","directory = \"/content/drive/MyDrive/Colab Notebooks/Restaurant_NER/Model\"\n","\n","if not os.path.exists(directory):\n","    os.makedirs(directory)\n","\n","# save vocabulary of the tokenizer\n","tokenizer.save_vocabulary(directory)\n","# save the model weights and its configuration file\n","model.save_pretrained(directory)\n","print('All files saved')\n","print('This tutorial is completed')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RjmzQLtcI__7","executionInfo":{"status":"ok","timestamp":1687179792005,"user_tz":-330,"elapsed":6354,"user":{"displayName":"Purchase - Techversant Infotech","userId":"04917963250138759673"}},"outputId":"5d0e7e27-de8a-4e0e-ea4a-738d985d1fb0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["All files saved\n","This tutorial is completed\n"]}]},{"cell_type":"markdown","source":["# Check for the output"],"metadata":{"id":"uCYfQ6lH2Lm1"}},{"cell_type":"code","source":["from transformers import pipeline\n","\n","pipe = pipeline(task=\"token-classification\", model=model.to(\"cpu\"), tokenizer=tokenizer, aggregation_strategy=\"simple\")\n","pipe(\"The restaurant is on upper Night Market close to the railway road.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u-YcVHn4MivB","executionInfo":{"status":"ok","timestamp":1687181380658,"user_tz":-330,"elapsed":10952,"user":{"displayName":"Purchase - Techversant Infotech","userId":"04917963250138759673"}},"outputId":"5dc4cff1-0974-4e02-89a8-4edecd94ceab"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'entity_group': 'RLOC',\n","  'score': 0.9963379,\n","  'word': 'market',\n","  'start': None,\n","  'end': None},\n"," {'entity_group': 'RLOC',\n","  'score': 0.9930742,\n","  'word': 'close to',\n","  'start': None,\n","  'end': None}]"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":[],"metadata":{"id":"PcwJcYKPNKJA"},"execution_count":null,"outputs":[]}]}